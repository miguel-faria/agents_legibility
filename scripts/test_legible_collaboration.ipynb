{
 "cells": [
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Callable, Optional\n",
    "from gymnasium.spaces import MultiBinary\n",
    "from agents.agent import Agent\n",
    "\n",
    "from dl_algos.dqn import DQNetwork\n",
    "from logging import Logger\n",
    "\n",
    "LEADER_ID = 0\n",
    "TOM_ID = 1\n",
    "RNG_SEED = 20240729\n",
    "CONF = 1.0\n",
    "\n",
    "def is_deadlock(history: List, new_state: str, last_actions: Tuple) -> bool:\n",
    "\n",
    "\tif len(history) < 3:\n",
    "\t\treturn False\n",
    "\n",
    "\tdeadlock = True\n",
    "\t# if all([act == Action.NONE for act in last_actions]) or all([act == Action.LOAD for act in last_actions]):\n",
    "\t# \treturn False\n",
    "\t#\n",
    "\t# else:\n",
    "\tstate_repitition = 0\n",
    "\tfor state in history:\n",
    "\t\tif new_state == state:\n",
    "\t\t\tstate_repitition += 1\n",
    "\tif state_repitition < 3:\n",
    "\t\tdeadlock = False\n",
    "\n",
    "\treturn deadlock\n"
   ],
   "id": "4852c96fdf42681e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# LB-Foraging",
   "id": "86784a85f264f28a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from agents.tom_agent import TomAgent\n",
    "from dl_envs.lb_foraging.lb_foraging_coop import FoodCOOPLBForaging\n",
    "from dl_envs.lb_foraging.lb_foraging import Action, Direction\n",
    "\n",
    "def coordinate_agents(env: FoodCOOPLBForaging, predict_task: str, actions: Tuple[int, int]) -> Tuple[int, int]:\n",
    "\n",
    "\tobjective = env.obj_food\n",
    "\tplayer_pos = [player.position for player in env.players]\n",
    "\tobjective_adj = env.get_adj_pos(objective[0], objective[1])\n",
    "\n",
    "\tif all([pos in objective_adj for pos in player_pos]):\n",
    "\t\tif predict_task == str(objective):\n",
    "\t\t\treturn Action.LOAD, Action.LOAD\n",
    "\t\telse:\n",
    "\t\t\treturn actions\n",
    "\n",
    "\telse:\n",
    "\t\tleader_pos = player_pos[LEADER_ID]\n",
    "\t\ttom_pos = player_pos[TOM_ID]\n",
    "\t\tlead_direction = Direction[Action(actions[LEADER_ID]).name].value\n",
    "\t\ttom_direction = Direction[Action(actions[TOM_ID]).name].value\n",
    "\t\tnext_lead_pos = (leader_pos[0] + lead_direction[0], leader_pos[1] + lead_direction[1])\n",
    "\t\tnext_tom_pos = (tom_pos[0] + tom_direction[0], tom_pos[1] + tom_direction[1])\n",
    "\t\tif next_lead_pos == next_tom_pos or all([act == Action.LOAD for act in actions]):\n",
    "\t\t\treturn actions[LEADER_ID], Action.NONE.value\n",
    "\t\telse:\n",
    "\t\t\treturn actions\n",
    "\n",
    "\n",
    "def load_models(opt_models_dir: Path, leg_models_dir: Path, n_foods_spawn: int, food_locs: List[Tuple], foods_lvl: int, num_layers: int, act_function: Callable,\n",
    "                layer_sizes: List[int], gamma: float, use_cnn: bool, use_dueling: bool, use_ddqn: bool, cnn_shape: Tuple, cnn_properties: List = None) -> Tuple[Dict, Dict]:\n",
    "\toptim_models = {}\n",
    "\tleg_models = {}\n",
    "\topt_model_names = [fname.name for fname in (opt_models_dir / ('%d-foods_%d-food-level' % (n_foods_spawn, foods_lvl)) / 'best').iterdir()]\n",
    "\tleg_model_names = [fname.name for fname in (leg_models_dir / ('%d-foods_%d-food-level' % (n_foods_spawn, foods_lvl)) / 'best').iterdir()]\n",
    "\ttry:\n",
    "\t\tfor loc in food_locs:\n",
    "\t\t\t# Find the optimal model name for the food location\n",
    "\t\t\tmodel_name = ''\n",
    "\t\t\tfor name in opt_model_names:\n",
    "\t\t\t\tif name.find(\"%sx%s\" % (loc[0], loc[1])) != -1:\n",
    "\t\t\t\t\tmodel_name = name\n",
    "\t\t\t\t\tbreak\n",
    "\t\t\tassert model_name != ''\n",
    "\t\t\topt_dqn = DQNetwork(len(Action), num_layers, act_function, layer_sizes, gamma, use_dueling, use_ddqn, use_cnn, cnn_properties)\n",
    "\t\t\topt_dqn.load_model(model_name, opt_models_dir / ('%d-foods_%d-food-level' % (n_foods_spawn, foods_lvl)) / 'best', None, cnn_shape, True)\n",
    "\t\t\toptim_models[str(loc)] = opt_dqn\n",
    "\n",
    "\t\t\t# Find the legible model name for the food location\n",
    "\t\t\tmodel_name = ''\n",
    "\t\t\tfor name in leg_model_names:\n",
    "\t\t\t\tif name.find(\"%sx%s\" % (loc[0], loc[1])) != -1:\n",
    "\t\t\t\t\tmodel_name = name\n",
    "\t\t\t\t\tbreak\n",
    "\t\t\tassert model_name != ''\n",
    "\t\t\tleg_dqn = DQNetwork(len(Action), num_layers, act_function, layer_sizes, gamma, use_dueling, use_ddqn, use_cnn, cnn_properties)\n",
    "\t\t\tleg_dqn.load_model(model_name, leg_models_dir / ('%d-foods_%d-food-level' % (n_foods_spawn, foods_lvl)) / 'best', None, cnn_shape, True)\n",
    "\t\t\tleg_models[str(loc)] = leg_dqn\n",
    "\n",
    "\t\treturn optim_models, leg_models\n",
    "\n",
    "\texcept AssertionError as e:\n",
    "\t\tprint(e)\n",
    "\t\treturn {}, {}\n",
    "\n",
    "def eval_legibility(n_runs: int, test_mode: int, opt_models_dir: Path, leg_models_dir: Path, field_dims: Tuple[int, int], n_agents: int,\n",
    "                    player_level: int, player_sight: int, max_foods: int, max_foods_spawn: int, food_locs: List[Tuple], foods_lvl: int, max_steps: int, gamma: float,\n",
    "                    num_layers: int, act_function: Callable, layer_sizes: List[int], use_cnn: bool, use_dueling: bool, use_ddqn: bool,\n",
    "                    cnn_properties: List = None, use_render: bool = False, start_run: int = 0):\n",
    "\n",
    "\tenv = FoodCOOPLBForaging(n_agents, player_level, field_dims, max_foods, player_sight, max_steps, True, foods_lvl, RNG_SEED, food_locs, use_render=use_render,\n",
    "\t                         use_encoding=True, agent_center=True, grid_observation=use_cnn)\n",
    "\tif isinstance(env.observation_space, MultiBinary):\n",
    "\t\tobs_space = MultiBinary([*env.observation_space.shape[1:]])\n",
    "\telse:\n",
    "\t\tobs_space = env.observation_space[0]\n",
    "\tcnn_shape = (0,) if not use_cnn else (*obs_space.shape[1:], obs_space.shape[0])\n",
    "\n",
    "\tstart_optim_models, start_leg_models = load_models(opt_models_dir, leg_models_dir, max_foods_spawn, food_locs, foods_lvl, num_layers, act_function, layer_sizes, gamma, use_cnn,\n",
    "\t                                       use_dueling, use_ddqn, cnn_shape, cnn_properties)\n",
    "\tresults = {}\n",
    "\tfor run_nr in range(start_run, n_runs):\n",
    "\n",
    "\t\trng_seed = RNG_SEED + run_nr\n",
    "\t\t# Initialize the agents for the interaction\n",
    "\t\tif test_mode == 0:\n",
    "\t\t\tleader_agent = Agent(LEADER_ID, start_optim_models, rng_seed)\n",
    "\t\t\ttom_agent = TomAgent(TOM_ID, start_optim_models, start_optim_models, rng_seed, 1)\n",
    "\t\telif test_mode == 1:\n",
    "\t\t\tleader_agent = Agent(LEADER_ID, start_optim_models, rng_seed)\n",
    "\t\t\ttom_agent = TomAgent(TOM_ID, start_leg_models, start_optim_models, rng_seed, 1)\n",
    "\t\telif test_mode == 2:\n",
    "\t\t\tleader_agent = Agent(LEADER_ID, start_leg_models, rng_seed)\n",
    "\t\t\ttom_agent = TomAgent(TOM_ID, start_optim_models, start_leg_models, rng_seed, 1)\n",
    "\t\telse:\n",
    "\t\t\tleader_agent = Agent(LEADER_ID, start_leg_models, rng_seed)\n",
    "\t\t\ttom_agent = TomAgent(TOM_ID, start_leg_models, start_leg_models, rng_seed, 1)\n",
    "\n",
    "\t\tenv = FoodCOOPLBForaging(n_agents, player_level, field_dims, max_foods, player_sight, max_steps, True, foods_lvl, rng_seed, food_locs, use_render=use_render,\n",
    "\t\t\t\t\t\t\t\t use_encoding=True, agent_center=True, grid_observation=use_cnn)\n",
    "\t\tit_results = {}\n",
    "\t\trng_gen = np.random.default_rng(rng_seed)\n",
    "\t\tspawned_foods = [food_locs[idx] for idx in rng_gen.choice(max_foods, size=max_foods_spawn, replace=False)]\n",
    "\t\tfoods_left = spawned_foods.copy()\n",
    "\t\tn_foods_left = max_foods_spawn\n",
    "\t\tstart_obj = foods_left.pop(rng_gen.integers(max_foods_spawn))\n",
    "\t\ttask = str(start_obj)\n",
    "\n",
    "\t\t# Setup agents for test\n",
    "\t\ttasks = [str(food) for food in spawned_foods]\n",
    "\t\ttasks.sort()\n",
    "\t\tleader_agent.init_interaction(tasks)\n",
    "\t\ttom_agent.init_interaction(tasks)\n",
    "\n",
    "\t\t# Setup environment for test\n",
    "\t\tenv.food_spawn_pos = spawned_foods\n",
    "\t\tenv.n_food_spawn = max_foods_spawn\n",
    "\t\tenv.set_objective(start_obj)\n",
    "\t\tenv.spawn_players()\n",
    "\t\tenv.spawn_food(max_foods_spawn, foods_lvl)\n",
    "\t\tif isinstance(env.observation_space, MultiBinary):\n",
    "\t\t\tobs_space = MultiBinary([*env.observation_space.shape[1:]])\n",
    "\t\telse:\n",
    "\t\t\tobs_space = env.observation_space[0]\n",
    "\t\tcnn_shape = (0,) if not use_cnn else (*obs_space.shape[1:], obs_space.shape[0])\n",
    "\t\tobs, *_ = env.reset()\n",
    "\n",
    "\t\trecent_states = [''.join([''.join(str(x) for x in p.position) for p in env.players]) + ''.join([''.join(str(x) for x in f.position) for f in env.foods])]\n",
    "\t\tif use_cnn:\n",
    "\t\t\tleader_obs = obs[0].reshape((1, *cnn_shape))\n",
    "\t\t\ttom_obs = obs[1].reshape((1, *cnn_shape))\n",
    "\t\telse:\n",
    "\t\t\tleader_obs = obs[0]\n",
    "\t\t\ttom_obs = obs[1]\n",
    "\t\tactions = (leader_agent.action(leader_obs, (leader_obs, Action.NONE), CONF, None, task),\n",
    "\t\t\t\t   tom_agent.action(tom_obs, (leader_obs, Action.NONE), CONF, None, tom_agent.predict_task))\n",
    "\n",
    "\t\ttimeout = False\n",
    "\t\tn_steps = 0\n",
    "\t\tn_pred_steps = []\n",
    "\t\tsteps_food = []\n",
    "\t\tdeadlock_states = []\n",
    "\t\tn_deadlocks = 0\n",
    "\t\tact_try = 0\n",
    "\t\tlater_error = 0\n",
    "\t\tlater_food_step = 0\n",
    "\n",
    "\t\tif use_render:\n",
    "\t\t\tenv.render()\n",
    "\n",
    "\t\tprint('Started run number %d:' % (run_nr + 1))\n",
    "\t\tprint(env.get_full_env_log())\n",
    "\t\twhile n_foods_left > 1 and not timeout:\n",
    "\t\t\tprint('Run number %d, step %d: remaining %d foods, predicted objective %s and real objective %s from ' % (run_nr + 1, n_steps + 1, n_foods_left, tom_agent.predict_task, task) + str(foods_left))\n",
    "\t\t\tn_steps += 1\n",
    "\t\t\tlast_leader_sample = (leader_obs, actions[0])\n",
    "\t\t\tif task != tom_agent.predict_task:\n",
    "\t\t\t\tlater_error = n_steps\n",
    "\t\t\tobs, _, _, timeout, _ = env.step(actions)\n",
    "\t\t\tif use_render:\n",
    "\t\t\t\tenv.render()\n",
    "\t\t\tcurrent_food_count = np.sum([not food.picked for food in env.foods])\n",
    "\n",
    "\t\t\tif use_cnn:\n",
    "\t\t\t\tleader_obs = obs[0].reshape((1, *cnn_shape))\n",
    "\t\t\t\ttom_obs = obs[1].reshape((1, *cnn_shape))\n",
    "\t\t\telse:\n",
    "\t\t\t\tleader_obs = obs[0]\n",
    "\t\t\t\ttom_obs = obs[1]\n",
    "\n",
    "\t\t\tif timeout:\n",
    "\t\t\t\tn_pred_steps += [later_error - later_food_step]\n",
    "\t\t\t\tsteps_food += [n_steps - later_food_step]\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\t\telif current_food_count < n_foods_left:\n",
    "\t\t\t\tn_foods_left = current_food_count\n",
    "\t\t\t\tn_pred_steps += [later_error - later_food_step]\n",
    "\t\t\t\tsteps_food += [n_steps - later_food_step]\n",
    "\t\t\t\tlater_food_step = n_steps\n",
    "\t\t\t\tlater_error = n_steps\n",
    "\n",
    "\t\t\t\tif current_food_count > 0:\n",
    "\t\t\t\t\t# Update tasks remaining and samples\n",
    "\t\t\t\t\ttasks = [str(food) for food in foods_left]\n",
    "\t\t\t\t\ttasks.sort()\n",
    "\t\t\t\t\ttom_agent.reset_inference(tasks)\n",
    "\t\t\t\t\tlast_leader_sample = (leader_obs, Action.NONE)\n",
    "\t\t\t\t\trecent_states = []\n",
    "\n",
    "\t\t\t\t\t# Update decision models\n",
    "\t\t\t\t\toptim_models, leg_models = load_models(opt_models_dir, leg_models_dir, n_foods_left, food_locs, foods_lvl, num_layers, act_function, layer_sizes,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t   gamma, use_cnn, use_dueling, use_ddqn, cnn_shape, cnn_properties)\n",
    "\t\t\t\t\tif test_mode == 0:\n",
    "\t\t\t\t\t\tleader_agent.goal_models = optim_models\n",
    "\t\t\t\t\t\ttom_agent.goal_models = optim_models\n",
    "\t\t\t\t\t\ttom_agent.sample_models = optim_models\n",
    "\t\t\t\t\telif test_mode == 1:\n",
    "\t\t\t\t\t\tleader_agent.goal_models = optim_models\n",
    "\t\t\t\t\t\ttom_agent.goal_models = leg_models\n",
    "\t\t\t\t\t\ttom_agent.sample_models = optim_models\n",
    "\t\t\t\t\telif test_mode == 2:\n",
    "\t\t\t\t\t\tleader_agent.goal_models = leg_models\n",
    "\t\t\t\t\t\ttom_agent.goal_models = optim_models\n",
    "\t\t\t\t\t\ttom_agent.sample_models = leg_models\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tleader_agent.goal_models = leg_models\n",
    "\t\t\t\t\t\ttom_agent.goal_models = leg_models\n",
    "\t\t\t\t\t\ttom_agent.sample_models = leg_models\n",
    "\n",
    "\t\t\t\t\t# Get next objective\n",
    "\t\t\t\t\tnext_obj = foods_left.pop(rng_gen.integers(n_foods_left))\n",
    "\t\t\t\t\ttask = str(next_obj)\n",
    "\t\t\t\t\tenv.set_objective(next_obj)\n",
    "\n",
    "\t\t\tcurrent_state = ''.join([''.join(str(x) for x in p.position) for p in env.players]) + ''.join([''.join(str(x) for x in f.position) for f in env.foods])\n",
    "\t\t\tif is_deadlock(recent_states, current_state, actions):\n",
    "\t\t\t\tn_deadlocks += 1\n",
    "\t\t\t\tif current_state not in deadlock_states:\n",
    "\t\t\t\t\tdeadlock_states.append(current_state)\n",
    "\t\t\t\tact_try += 1\n",
    "\t\t\t\tactions = (leader_agent.sub_acting(leader_obs, None, act_try - 1, last_leader_sample, CONF, task),\n",
    "\t\t\t\t\t\t   tom_agent.sub_acting(tom_obs, None, act_try, last_leader_sample, CONF))\n",
    "\t\t\t\t# actions = (leader_agent.action(leader_obs, last_leader_sample, CONF, None, task), tom_agent.sub_acting(tom_obs, None, act_try, last_leader_sample, CONF))\n",
    "\t\t\telse:\n",
    "\t\t\t\tact_try = 0\n",
    "\t\t\t\tactions = (leader_agent.action(leader_obs, last_leader_sample, CONF, None, task), tom_agent.action(tom_obs, last_leader_sample, CONF, None))\n",
    "\n",
    "\t\t\tactions = coordinate_agents(env, tom_agent.predict_task, actions)\n",
    "\n",
    "\t\t\trecent_states.append(current_state)\n",
    "\t\t\tif len(recent_states) > 3:\n",
    "\t\t\t\trecent_states.pop(0)\n",
    "\n",
    "\t\tenv.close()\n",
    "\t\tprint('Run Over!!')\n",
    "\t\tit_results['n_steps'] = n_steps\n",
    "\t\tit_results['pred_steps'] = n_pred_steps\n",
    "\t\tit_results['avg_pred_steps'] = np.mean(n_pred_steps) if len(n_pred_steps) > 0 else 0\n",
    "\t\tit_results['caught_foods'] = max_foods_spawn - n_foods_left\n",
    "\t\tit_results['steps_food'] = steps_food\n",
    "\t\tit_results['deadlocks'] = n_deadlocks\n",
    "\t\tresults[run_nr] = it_results\n"
   ],
   "id": "e146e9309ea6210c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Pursuit-Prey",
   "id": "6172119b9c87c65c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from dl_envs.pursuit.pursuit_env import TargetPursuitEnv, Action, ActionDirection\n",
    "from dl_envs.pursuit.agents.random_prey import RandomPrey\n",
    "from dl_envs.pursuit.agents.greedy_prey import GreedyPrey\n",
    "from dl_envs.pursuit.agents.agent import Agent as PreyAgent\n",
    "\n",
    "PREY_TYPES = {'idle': 0, 'greedy': 1, 'random': 2}\n",
    "\n",
    "\n",
    "class TomAgent(Agent):\n",
    "\n",
    "\t_goal_prob: jnp.ndarray\n",
    "\t_sample_models: Dict[str, DQNetwork]\n",
    "\t_interaction_likelihoods: jnp.ndarray\n",
    "\t_sign: float\n",
    "\t_predict_task: str\n",
    "\n",
    "\tdef __init__(self, agent_id: int, goal_models: Dict[str, DQNetwork], sample_models: Dict[str, DQNetwork], rng_seed: int = 1234567890, sign: float = -1):\n",
    "\n",
    "\t\tsuper().__init__(agent_id, goal_models, rng_seed)\n",
    "\t\tself._sample_models = sample_models\n",
    "\t\tself._goal_prob = jnp.array([])\n",
    "\t\tself._interaction_likelihoods = jnp.array([])\n",
    "\t\tself._sign = sign\n",
    "\t\tself._predict_task = ''\n",
    "\n",
    "\t@property\n",
    "\tdef goal_prob(self) -> jnp.ndarray:\n",
    "\t\treturn self._goal_prob\n",
    "\n",
    "\t@property\n",
    "\tdef interaction_likelihoods(self) -> jnp.ndarray:\n",
    "\t\treturn self._interaction_likelihoods\n",
    "\n",
    "\t@property\n",
    "\tdef predict_task(self) -> str:\n",
    "\t\treturn self._predict_task\n",
    "\n",
    "\t@property\n",
    "\tdef sample_models(self) -> Dict[str, DQNetwork]:\n",
    "\t\treturn self._sample_models\n",
    "\n",
    "\t@goal_prob.setter\n",
    "\tdef goal_prob(self, goal_prob: jnp.ndarray) -> None:\n",
    "\t\tself._goal_prob = goal_prob\n",
    "\n",
    "\t@sample_models.setter\n",
    "\tdef sample_models(self, sample_models: Dict[str, DQNetwork]) -> None:\n",
    "\t\tself._sample_models = sample_models\n",
    "\n",
    "\tdef add_sample_model(self, task: str, model: DQNetwork) -> None:\n",
    "\t\tself._sample_models[task] = model\n",
    "\n",
    "\tdef remove_sample_model(self, task: str) -> None:\n",
    "\t\tself._sample_models.pop(task)\n",
    "\n",
    "\tdef init_interaction(self, interaction_tasks: List[str]):\n",
    "\t\tself._tasks = interaction_tasks.copy()\n",
    "\t\tself._n_tasks = len(interaction_tasks)\n",
    "\t\tself._goal_prob = jnp.ones(self._n_tasks) / self._n_tasks\n",
    "\t\tself._interaction_likelihoods = jnp.ones(self._n_tasks)\n",
    "\t\tself._predict_task = interaction_tasks[0]\n",
    "\n",
    "\tdef reset_inference(self, tasks: List = None):\n",
    "\t\tif tasks:\n",
    "\t\t\tself._tasks = tasks.copy()\n",
    "\t\t\tself._n_tasks = len(self._tasks)\n",
    "\t\tself._interaction_likelihoods = jnp.ones(self._n_tasks)\n",
    "\t\tself._goal_prob = jnp.ones(self._n_tasks) / self._n_tasks\n",
    "\t\tself._predict_task = self._tasks[0]\n",
    "\n",
    "\tdef sample_probability(self, obs: jnp.ndarray, a: int, conf: float) -> jnp.ndarray:\n",
    "\t\tgoals_likelihood = []\n",
    "\t\tmodel_id = list(self._goal_models.keys())[0]\n",
    "\n",
    "\t\tfor task_idx in range(self._n_tasks):\n",
    "\t\t\tq = jax.device_get(self._sample_models[model_id].q_network.apply(self._sample_models[model_id].online_state.params, obs[task_idx])[0])\n",
    "\t\t\tgoals_likelihood += [jnp.exp(self._sign * conf * (q[a] - q.max())) / jnp.sum(jnp.exp(self._sign * conf * (q - q.max())))]\n",
    "\n",
    "\t\tgoals_likelihood = jnp.array(goals_likelihood)\n",
    "\t\treturn goals_likelihood\n",
    "\n",
    "\tdef task_inference(self) -> str:\n",
    "\t\tif not self._tasks:\n",
    "\t\t\tprint('[ERROR]: List of possible tasks not defined!!')\n",
    "\t\t\treturn ''\n",
    "\n",
    "\t\tif len(self._interaction_likelihoods) > 0:\n",
    "\t\t\tlikelihood = jnp.cumprod(jnp.array(self._interaction_likelihoods), axis=0)[-1]\n",
    "\t\telse:\n",
    "\t\t\tlikelihood = jnp.zeros(self._n_tasks)\n",
    "\t\tgoals_prob = self._goal_prob * likelihood\n",
    "\t\tgoals_prob_sum = goals_prob.sum()\n",
    "\t\tif goals_prob_sum == 0:\n",
    "\t\t\tp_max = jnp.ones(self._n_tasks) / self._n_tasks\n",
    "\t\telse:\n",
    "\t\t\tp_max = goals_prob / goals_prob_sum\n",
    "\t\thigh_likelihood = jnp.argwhere(p_max == jnp.amax(p_max)).ravel()\n",
    "\t\tself._rng_key, subkey = jax.random.split(self._rng_key)\n",
    "\t\treturn self._tasks[jax.random.choice(subkey, high_likelihood)]\n",
    "\n",
    "\tdef bayesian_task_inference(self, sample: Tuple[jnp.ndarray, int], conf: float) -> Tuple[str, float]:\n",
    "\n",
    "\t\tif not self._tasks:\n",
    "\t\t\tprint('[ERROR]: List of possible tasks not defined!!')\n",
    "\t\t\treturn '', -1\n",
    "\n",
    "\t\tstates, action = sample\n",
    "\t\tsample_prob = self.sample_probability(states, action, conf)\n",
    "\t\tself._interaction_likelihoods = jnp.vstack((self._interaction_likelihoods, sample_prob))\n",
    "\n",
    "\t\tlikelihoods = jnp.cumprod(self._interaction_likelihoods, axis=0)[-1]\n",
    "\t\tgoals_prob = likelihoods * self._goal_prob\n",
    "\t\tgoals_prob_sum = goals_prob.sum()\n",
    "\t\tif goals_prob_sum == 0:\n",
    "\t\t\tp_max = jnp.ones(self._n_tasks) / self._n_tasks\n",
    "\t\telse:\n",
    "\t\t\tp_max = goals_prob / goals_prob_sum\n",
    "\t\tmax_idx = jnp.argwhere(p_max == jnp.amax(p_max)).ravel()\n",
    "\t\tself._rng_key, subkey = jax.random.split(self._rng_key)\n",
    "\t\tmax_task_prob = jax.random.choice(subkey, max_idx)\n",
    "\t\ttask_conf = float(p_max[max_task_prob])\n",
    "\t\ttask_id = self._tasks[max_task_prob]\n",
    "\n",
    "\t\treturn task_id, task_conf\n",
    "\n",
    "\tdef get_actions(self, task_id: str, obs: jnp.ndarray) -> int:\n",
    "\t\tmodel_id = list(self._goal_models.keys())[0]\n",
    "\t\tq = jax.device_get(self._goal_models[model_id].q_network.apply(self._goal_models[model_id].online_state.params, obs)[0])\n",
    "\t\tpol = jnp.isclose(q, q.max(), rtol=1e-10, atol=1e-10).astype(int)\n",
    "\t\tpol = pol / pol.sum()\n",
    "\t\t# print(self._agent_id, task_id, q, q - q.max(), pol)\n",
    "\n",
    "\t\tself._rng_key, subkey = jax.random.split(self._rng_key)\n",
    "\t\treturn int(jax.random.choice(subkey, len(q), p=pol))\n",
    "\n",
    "\tdef action(self, obs: jnp.ndarray, sample: Tuple[jnp.ndarray, int], conf: float, logger: Optional[Logger], task: str = '') -> int:\n",
    "\t\tpredict_task, predict_conf = self.bayesian_task_inference(sample, conf)\n",
    "\t\tself._predict_task = predict_task\n",
    "\t\treturn self.get_actions(self._predict_task, obs)\n",
    "\n",
    "\tdef sub_acting(self, obs: jnp.ndarray, logger: Optional[Logger], act_try: int, sample: Tuple[jnp.ndarray, int], conf: float, task: str = '') -> int:\n",
    "\t\tpredict_task, predict_conf = self.bayesian_task_inference(sample, conf)\n",
    "\t\tself._predict_task = predict_task\n",
    "\t\treturn super().sub_acting(obs, logger, act_try, sample, conf, self._predict_task if task == '' else task)\n",
    "\n",
    "\n",
    "def coordinate_agents(env: TargetPursuitEnv, predict_task: str, actions: Tuple[int], n_tom_agents: int) -> Tuple[int]:\n",
    "\n",
    "\tobjective = env.target\n",
    "\thunter_pos = [env.agents[h_id].pos for h_id in env.hunter_ids]\n",
    "\tobjective_adj = env.adj_pos(env.agents[objective].pos)\n",
    "\n",
    "\tif sum([pos in objective_adj for pos in hunter_pos]) >= env.n_catch:\n",
    "\t\tif predict_task == str(objective):\n",
    "\t\t\treturn tuple([Action.STAY] * env.n_hunters)\n",
    "\t\telse:\n",
    "\t\t\treturn actions\n",
    "\n",
    "\telse:\n",
    "\t\tleader_pos = hunter_pos[LEADER_ID]\n",
    "\t\tlead_direction = ActionDirection[Action(actions[LEADER_ID]).name].value\n",
    "\t\tnext_lead_pos = (leader_pos[0] + lead_direction[0], leader_pos[1] + lead_direction[1])\n",
    "\t\ttom_pos = []\n",
    "\t\ttom_directions = []\n",
    "\t\tnext_tom_pos = []\n",
    "\t\tfor idx in range(n_tom_agents):\n",
    "\t\t\ttom_pos.append(hunter_pos[TOM_ID + idx])\n",
    "\t\t\ttom_directions.append(ActionDirection[Action(actions[TOM_ID + idx]).name].value)\n",
    "\t\t\tnext_tom_pos.append((tom_pos[-1][0] + tom_directions[-1][0], tom_pos[-1][1] + tom_directions[-1][1]))\n",
    "\n",
    "\t\tcoord_acts = actions\n",
    "\t\tfor idx in range(n_tom_agents):\n",
    "\t\t\tif next_tom_pos[idx] == next_lead_pos or (idx > 0 and next_tom_pos[idx] == next_tom_pos[idx - 1]):\n",
    "\t\t\t\tcoord_acts = coord_acts[:TOM_ID + idx] + (Action.STAY.value, ) + coord_acts[TOM_ID + idx + 1:]\n",
    "\n",
    "\t\treturn coord_acts\n",
    "\n",
    "\n",
    "def load_models(opt_models_dir: Path, leg_models_dir: Path, n_hunters: int, prey_type: str, n_preys_alive: int, num_layers: int, act_function: Callable,\n",
    "                layer_sizes: List[int], gamma: float, use_cnn: bool, use_dueling: bool, use_ddqn: bool, cnn_shape: Tuple, cnn_properties: List = None) -> Tuple[Dict, Dict]:\n",
    "\toptim_models = {}\n",
    "\tleg_models = {}\n",
    "\topt_model_names = [fname.name for fname in (opt_models_dir / ('%d-hunters' % n_hunters) / ('%s-prey' % prey_type) / 'best').iterdir()]\n",
    "\tleg_model_names = [fname.name for fname in (leg_models_dir / ('%d-hunters' % n_hunters) / ('%s-prey' % prey_type) / 'best').iterdir()]\n",
    "\ttry:\n",
    "\t\t# Find the optimal model name for the food location\n",
    "\t\tmodel_name = ''\n",
    "\t\tfor name in opt_model_names:\n",
    "\t\t\tif name.find(\"%d\" % n_preys_alive) != -1:\n",
    "\t\t\t\tmodel_name = name\n",
    "\t\t\t\tbreak\n",
    "\t\tassert model_name != ''\n",
    "\t\topt_dqn = DQNetwork(len(Action), num_layers, act_function, layer_sizes, gamma, use_dueling, use_ddqn, use_cnn, cnn_properties)\n",
    "\t\topt_dqn.load_model(model_name, opt_models_dir / ('%d-hunters' % n_hunters) / ('%s-prey' % prey_type)  / 'best', None, cnn_shape, True)\n",
    "\t\toptim_models['p%d' % n_preys_alive] = opt_dqn\n",
    "\n",
    "\t\t# Find the legible model name for the food location\n",
    "\t\tmodel_name = ''\n",
    "\t\tfor name in leg_model_names:\n",
    "\t\t\tif name.find(\"%d\" % n_preys_alive) != -1:\n",
    "\t\t\t\tmodel_name = name\n",
    "\t\t\t\tbreak\n",
    "\t\tassert model_name != ''\n",
    "\t\tleg_dqn = DQNetwork(len(Action), num_layers, act_function, layer_sizes, gamma, use_dueling, use_ddqn, use_cnn, cnn_properties)\n",
    "\t\tleg_dqn.load_model(model_name, leg_models_dir / ('%d-hunters' % n_hunters) / ('%s-prey' % prey_type) / 'best', None, cnn_shape, True)\n",
    "\t\tleg_models['p%d' % n_preys_alive] = leg_dqn\n",
    "\n",
    "\t\treturn optim_models, leg_models\n",
    "\n",
    "\texcept AssertionError as e:\n",
    "\t\tprint(e)\n",
    "\t\treturn {}, {}\n",
    "\n",
    "def eval_legibility(n_runs: int, test_mode: int, opt_models_dir: Path, leg_models_dir: Path, field_dims: Tuple[int, int], hunters: List[Tuple[str, int]],\n",
    "                    preys: List[Tuple[str, int]], player_sight: int, prey_ids: List[str], prey_type: str, require_catch: bool, catch_reward: float, max_steps: int, gamma: float,\n",
    "                    num_layers: int, act_function: Callable, layer_sizes: List[int], use_cnn: bool, use_dueling: bool, use_ddqn: bool,\n",
    "                    cnn_properties: List = None, use_render: bool = False, start_run: int = 0):\n",
    "\n",
    "\tenv = TargetPursuitEnv(hunters, preys, field_dims, player_sight, prey_ids[0], require_catch, max_steps, use_layer_obs=True, agent_centered=True, catch_reward=catch_reward)\n",
    "\tif isinstance(env.observation_space, MultiBinary):\n",
    "\t\tobs_space = MultiBinary([*env.observation_space.shape[1:]])\n",
    "\telse:\n",
    "\t\tobs_space = env.observation_space[0]\n",
    "\tcnn_shape = (0,) if not use_cnn else (*obs_space.shape[1:], obs_space.shape[0])\n",
    "\n",
    "\tstart_optim_models, start_leg_models = load_models(opt_models_dir, leg_models_dir, env.n_hunters, prey_type, env.n_preys, num_layers, act_function, layer_sizes, gamma, use_cnn,\n",
    "\t                                       use_dueling, use_ddqn, cnn_shape, cnn_properties)\n",
    "\tresults = {}\n",
    "\tfor run_nr in range(start_run, n_runs):\n",
    "\n",
    "\t\trng_seed = RNG_SEED + run_nr\n",
    "\t\t# Initialize the agents for the interaction\n",
    "\t\tn_hunters = len(hunters)\n",
    "\t\tn_tom_hunters = n_hunters - 1\n",
    "\t\tn_preys = len(preys)\n",
    "\t\tif test_mode == 0:\n",
    "\t\t\tleader_agent = Agent(LEADER_ID, start_optim_models, rng_seed)\n",
    "\t\t\ttom_agents = [TomAgent(TOM_ID + idx, start_optim_models, start_optim_models, rng_seed, 1) for idx in range(n_tom_hunters)]\n",
    "\t\telif test_mode == 1:\n",
    "\t\t\tleader_agent = Agent(LEADER_ID, start_optim_models, rng_seed)\n",
    "\t\t\ttom_agents = [TomAgent(TOM_ID + idx, start_leg_models, start_optim_models, rng_seed, 1) for idx in range(n_tom_hunters)]\n",
    "\t\telif test_mode == 2:\n",
    "\t\t\tleader_agent = Agent(LEADER_ID, start_leg_models, rng_seed)\n",
    "\t\t\ttom_agents = [TomAgent(TOM_ID + idx, start_optim_models, start_leg_models, rng_seed, 1) for idx in range(n_tom_hunters)]\n",
    "\t\telse:\n",
    "\t\t\tleader_agent = Agent(LEADER_ID, start_leg_models, rng_seed)\n",
    "\t\t\ttom_agents = [TomAgent(TOM_ID + idx, start_leg_models, start_leg_models, rng_seed, 1) for idx in range(n_tom_hunters)]\n",
    "\n",
    "\t\tenv = TargetPursuitEnv(hunters, preys, field_dims, player_sight, prey_ids[0], require_catch, max_steps, use_layer_obs=True, agent_centered=True, catch_reward=catch_reward)\n",
    "\t\tenv.seed(rng_seed)\n",
    "\t\tit_results = {}\n",
    "\t\trng_gen = np.random.default_rng(rng_seed)\n",
    "\n",
    "\t\t# Setup agents for test\n",
    "\t\tpreys_left = prey_ids.copy()\n",
    "\t\ttask = preys_left.pop(rng_gen.integers(n_preys))\n",
    "\t\ttasks = prey_ids.copy()\n",
    "\t\ttasks.sort()\n",
    "\t\tleader_agent.init_interaction(tasks)\n",
    "\t\tfor idx in range(n_tom_hunters):\n",
    "\t\t\ttom_agents[idx].init_interaction(tasks)\n",
    "\t\tprey_agents = {}\n",
    "\t\tfor i, prey_id in enumerate(prey_ids):\n",
    "\t\t\tif prey_type == 'random':\n",
    "\t\t\t\tprey_agents[prey_id] = RandomPrey(prey_id, 2, 0, rng_seed + i)\n",
    "\t\t\telif prey_type == 'greedy':\n",
    "\t\t\t\tprey_agents[prey_id] = GreedyPrey(prey_id, 2, 0, rng_seed + i)\n",
    "\t\t\telse:\n",
    "\t\t\t\tprey_agents[prey_id] = PreyAgent(prey_id, 2, 0, rng_seed + i)\n",
    "\n",
    "\t\t# Setup environment for test\n",
    "\t\tenv.reset_init_pos()\n",
    "\t\tenv.target = task\n",
    "\t\tif isinstance(env.observation_space, MultiBinary):\n",
    "\t\t\tobs_space = MultiBinary([*env.observation_space.shape[1:]])\n",
    "\t\telse:\n",
    "\t\t\tobs_space = env.observation_space[0]\n",
    "\t\tcnn_shape = (0,) if not use_cnn else (*obs_space.shape[1:], obs_space.shape[0])\n",
    "\t\tn_preys_alive = n_preys\n",
    "\t\tobs, *_ = env.reset()\n",
    "\n",
    "\t\trecent_states = [''.join([''.join(str(x) for x in env.agents[a_id].pos) for a_id in env.agents.keys() if env.agents[a_id].alive])]\n",
    "\t\tif use_cnn:\n",
    "\t\t\tleader_obs = obs[LEADER_ID].reshape((1, *cnn_shape))\n",
    "\t\t\tleader_sample = [env.make_target_grid_obs(prey)[LEADER_ID].reshape((1, *cnn_shape))  for prey in env.prey_alive_ids]\n",
    "\t\t\ttom_obs = [[env.make_target_grid_obs(prey)[idx + 1].reshape((1, *cnn_shape)) for prey in env.prey_alive_ids if prey == tom_agents[idx].predict_task][0] for idx in range(n_tom_hunters)]\n",
    "\t\telse:\n",
    "\t\t\tleader_obs = obs[LEADER_ID]\n",
    "\t\t\tleader_sample = [env.make_target_grid_obs(prey)[LEADER_ID] for prey in env.prey_alive_ids]\n",
    "\t\t\ttom_obs = [[env.make_target_grid_obs(prey)[idx + 1] for prey in env.prey_alive_ids if prey == tom_agents[idx].predict_task][0] for idx in range(n_tom_hunters)]\n",
    "\t\tactions = (leader_agent.action(leader_obs, (leader_sample, Action.STAY), CONF, None, 'p%d' % n_preys_alive),\n",
    "\t\t\t\t   *[tom_agents[idx].action(tom_obs[idx], (leader_sample, Action.STAY), CONF, None, 'p%d' % n_preys_alive) for idx in range(n_tom_hunters)])\n",
    "\n",
    "\t\tfor prey_id in env.prey_alive_ids:\n",
    "\t\t\tactions += (prey_agents[prey_id].act(env), )\n",
    "\n",
    "\t\ttimeout = False\n",
    "\t\tn_steps = 0\n",
    "\t\tn_pred_steps = []\n",
    "\t\tsteps_capture = []\n",
    "\t\tdeadlock_states = []\n",
    "\t\tn_deadlocks = 0\n",
    "\t\tact_try = 0\n",
    "\t\tlater_error = 0\n",
    "\t\tlater_food_step = 0\n",
    "\n",
    "\t\tif use_render:\n",
    "\t\t\tenv.render()\n",
    "\n",
    "\t\tprint('Started run number %d:' % (run_nr + 1))\n",
    "\t\tprint(env.get_full_env_log())\n",
    "\t\twhile n_preys_alive > 1 and not timeout:\n",
    "\t\t\tpredicted_objectives = ','.join(['%s for tom agent %d' % (tom_agents[idx].predict_task, tom_agents[idx].agent_id) for idx in range(n_tom_hunters)])\n",
    "\t\t\tprint('Run number %d, step %d: remaining %d preys, predicted objective %s and real objective %s from ' % (run_nr + 1, n_steps + 1,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  env.n_preys_alive, predicted_objectives, task) + ', '.join(env.prey_alive_ids))\n",
    "\t\t\tn_steps += 1\n",
    "\t\t\tif use_cnn:\n",
    "\t\t\t\tlast_leader_sample = ([env.make_target_grid_obs(prey)[LEADER_ID].reshape((1, *cnn_shape)) for prey in env.prey_alive_ids], actions[LEADER_ID])\n",
    "\t\t\telse:\n",
    "\t\t\t\tlast_leader_sample = ([env.make_target_grid_obs(prey)[LEADER_ID] for prey in env.prey_alive_ids], actions[LEADER_ID])\n",
    "\t\t\tif any([task != tom_agents[idx].predict_task for idx in range(n_tom_hunters)]):\n",
    "\t\t\t\tlater_error = n_steps\n",
    "\t\t\tprint('Actions: %s' % ', '.join([str(Action(action).name) for action in actions]))\n",
    "\t\t\tobs, _, _, timeout, _ = env.step(actions)\n",
    "\t\t\tif use_render:\n",
    "\t\t\t\tenv.render()\n",
    "\n",
    "\t\t\tif timeout:\n",
    "\t\t\t\tn_pred_steps += [later_error - later_food_step]\n",
    "\t\t\t\tsteps_capture += [n_steps - later_food_step]\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\t\telif env.n_preys_alive < n_preys_alive:\n",
    "\t\t\t\tn_preys_alive = env.n_preys_alive\n",
    "\t\t\t\tn_pred_steps += [later_error - later_food_step]\n",
    "\t\t\t\tsteps_capture += [n_steps - later_food_step]\n",
    "\t\t\t\tlater_food_step = n_steps\n",
    "\t\t\t\tlater_error = n_steps\n",
    "\n",
    "\t\t\t\tif env.n_preys_alive > 0:\n",
    "\t\t\t\t\t# Update tasks remaining and samples\n",
    "\t\t\t\t\ttasks = env.prey_alive_ids.copy()\n",
    "\t\t\t\t\ttasks.sort()\n",
    "\t\t\t\t\tfor idx in range(n_tom_hunters):\n",
    "\t\t\t\t\t\ttom_agents[idx].init_interaction(tasks)\n",
    "\t\t\t\t\tif use_cnn:\n",
    "\t\t\t\t\t\tlast_leader_sample = ([env.make_target_grid_obs(prey)[LEADER_ID].reshape((1, *cnn_shape))  for prey in env.prey_alive_ids], Action.STAY)\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tlast_leader_sample = ([env.make_target_grid_obs(prey)[LEADER_ID] for prey in env.prey_alive_ids], Action.STAY)\n",
    "\t\t\t\t\trecent_states = []\n",
    "\n",
    "\t\t\t\t\t# Update decision models\n",
    "\t\t\t\t\toptim_models, leg_models = load_models(opt_models_dir, leg_models_dir, n_hunters, prey_type, n_preys_alive, num_layers, act_function, layer_sizes,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t   gamma, use_cnn, use_dueling, use_ddqn, cnn_shape, cnn_properties)\n",
    "\t\t\t\t\tif test_mode == 0:\n",
    "\t\t\t\t\t\tleader_agent.goal_models = optim_models\n",
    "\t\t\t\t\t\tfor idx in range(n_tom_hunters):\n",
    "\t\t\t\t\t\t\ttom_agents[idx].goal_models = optim_models\n",
    "\t\t\t\t\t\t\ttom_agents[idx].sample_models = optim_models\n",
    "\t\t\t\t\telif test_mode == 1:\n",
    "\t\t\t\t\t\tleader_agent.goal_models = optim_models\n",
    "\t\t\t\t\t\tfor idx in range(n_tom_hunters):\n",
    "\t\t\t\t\t\t\ttom_agents[idx].goal_models = leg_models\n",
    "\t\t\t\t\t\t\ttom_agents[idx].sample_models = optim_models\n",
    "\t\t\t\t\telif test_mode == 2:\n",
    "\t\t\t\t\t\tleader_agent.goal_models = leg_models\n",
    "\t\t\t\t\t\tfor idx in range(n_tom_hunters):\n",
    "\t\t\t\t\t\t\ttom_agents[idx].goal_models = optim_models\n",
    "\t\t\t\t\t\t\ttom_agents[idx].sample_models = leg_models\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tleader_agent.goal_models = leg_models\n",
    "\t\t\t\t\t\tfor idx in range(n_tom_hunters):\n",
    "\t\t\t\t\t\t\ttom_agents[idx].goal_models = leg_models\n",
    "\t\t\t\t\t\t\ttom_agents[idx].sample_models = leg_models\n",
    "\n",
    "\t\t\t\t\t# Get next objective\n",
    "\t\t\t\t\tpreys_left = env.prey_alive_ids.copy()\n",
    "\t\t\t\t\ttask = preys_left.pop(rng_gen.integers(n_preys_alive))\n",
    "\t\t\t\t\tenv.target = task\n",
    "\n",
    "\t\t\t# Update leader and ToM agents' observations\n",
    "\t\t\tprint('Preys alive: %s' % ', '.join([str(prey) for prey in env.prey_alive_ids]))\n",
    "\t\t\tif use_cnn:\n",
    "\t\t\t\tprint(obs[LEADER_ID][-1])\n",
    "\t\t\t\tleader_obs = obs[LEADER_ID].reshape((1, *cnn_shape))\n",
    "\t\t\t\tfor idx in range(n_tom_hunters):\n",
    "\t\t\t\t\tfor prey in env.prey_alive_ids:\n",
    "\t\t\t\t\t\tif prey == tom_agents[idx].predict_task:\n",
    "\t\t\t\t\t\t\t# for layer in env.make_target_grid_obs(prey)[idx + 1]:\n",
    "\t\t\t\t\t\t\t# \tprint(layer)\n",
    "\t\t\t\t\t\t\t# input()\n",
    "\t\t\t\t\t\t\ttom_obs[idx] = env.make_target_grid_obs(prey)[idx + 1].reshape((1, *cnn_shape))\n",
    "\t\t\telse:\n",
    "\t\t\t\tleader_obs = obs[LEADER_ID]\n",
    "\t\t\t\tfor idx in range(n_tom_hunters):\n",
    "\t\t\t\t\tfor prey in env.prey_alive_ids:\n",
    "\t\t\t\t\t\tif prey == tom_agents[idx].predict_task:\n",
    "\t\t\t\t\t\t\ttom_obs[idx] = env.make_target_grid_obs(prey)[idx + 1].reshape((1, *cnn_shape))\n",
    "\n",
    "\t\t\tcurrent_state = ''.join([''.join(str(x) for x in env.agents[a_id].pos) for a_id in env.agents.keys() if env.agents[a_id].alive])\n",
    "\t\t\tif is_deadlock(recent_states, current_state, actions):\n",
    "\t\t\t\tn_deadlocks += 1\n",
    "\t\t\t\tif current_state not in deadlock_states:\n",
    "\t\t\t\t\tdeadlock_states.append(current_state)\n",
    "\t\t\t\tact_try += 1\n",
    "\t\t\t\tactions = (leader_agent.sub_acting(leader_obs, None, act_try - 1, last_leader_sample, CONF, 'p%d' % n_preys_alive),\n",
    "\t\t\t\t\t\t   *[tom_agents[idx].sub_acting(tom_obs[idx], None, act_try, last_leader_sample, CONF, 'p%d' % n_preys_alive)for idx in range(n_tom_hunters)])\n",
    "\t\t\t\t# actions = (leader_agent.action(leader_obs, last_leader_sample, CONF, None, task), tom_agent.sub_acting(tom_obs, None, act_try, last_leader_sample, CONF))\n",
    "\t\t\telse:\n",
    "\t\t\t\tact_try = 0\n",
    "\t\t\t\tactions = (leader_agent.action(leader_obs, last_leader_sample, CONF, None, 'p%d' % n_preys_alive),\n",
    "\t\t\t\t\t\t   *[tom_agents[idx].action(tom_obs[idx], last_leader_sample, CONF, None, 'p%d' % n_preys_alive) for idx in range(n_tom_hunters)])\n",
    "\n",
    "\t\t\tactions = coordinate_agents(env, [tom_agents[idx].predict_task for idx in range(n_tom_hunters)], actions, n_tom_hunters)\n",
    "\t\t\tfor prey_id in env.prey_ids:\n",
    "\t\t\t\tactions += (prey_agents[prey_id].act(env) if prey_id in env.prey_alive_ids else Action.STAY.value, )\n",
    "\n",
    "\t\t\trecent_states.append(current_state)\n",
    "\t\t\tif len(recent_states) > 3:\n",
    "\t\t\t\trecent_states.pop(0)\n",
    "\n",
    "\t\tenv.close()\n",
    "\t\tprint('Run Over!!')\n",
    "\t\tit_results['n_steps'] = n_steps\n",
    "\t\tit_results['pred_steps'] = n_pred_steps\n",
    "\t\tit_results['avg_pred_steps'] = np.mean(n_pred_steps) if len(n_pred_steps) > 0 else 0\n",
    "\t\tit_results['preys_captured'] = n_preys - n_preys_alive\n",
    "\t\tit_results['steps_capture'] = steps_capture\n",
    "\t\tit_results['deadlocks'] = n_deadlocks\n",
    "\t\tresults[run_nr] = it_results\n"
   ],
   "id": "1f36a4224d7f03a0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
